{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSI 15 Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Image Classification on Amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E-commerce sites have thousands of listings everyday and at times, the users may not be correctly classifying their uploaded image or be using the wrong product depiction. Mismatch of product listing information will decrease the effectiveness of succssful transactions and also result in unnecessary resources being utilitzed to perform these corrections on a large scale. A product detection system would help to ensure the correct listing and categorization of products or to assist the user in classifying product types. To meet this need, an image classifer will be trained and developed to accurately identify the correct image labels with the use of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This capstone project would be aimed at building an image classification model using convulated neural networks that will identify the following 3 categories of products: clothing, footwear and watches. The source data to construct this model will be based on images scraped from Amazon, the world's largest online retailer. \n",
    "\n",
    "Stakeholders will be the e-commerce companies and the user of the services themselves. It will help the company improve the effectiveness of potential transactions. It will also improve the user experience with more accuracy and also to avoid problems arising from wrongly identifying products.\n",
    "\n",
    "Metrics used to measure the performance would be the AUC & Type I / Type II errors.\n",
    "\n",
    "Challenges foreseen would be potential imbalanced data, complex background noise or poor resolution images.  \n",
    "\n",
    "The goal at present seems to be sufficiently scoped as there are 3 categoeires of distinct image features an the quantity of images are adequate for the purposes of this analysis. The timeline for completion tentatively end of July 2020 is still a reasonable expectation to work towards to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium and chrome webdriver is used to scrape the 3 categories in Amazon. The image formats are known to be fixed in size and hence will be saved as JPEG format. To ensure a sufficient quantity for testing and to account for possible discarding of ineligible datapoints, 10,000 images were saved from each class. They were extracted to the respective folders:\n",
    "<ul>\n",
    "    <li>Watch_Images</li>\n",
    "    <li>Shirt_Images</li>\n",
    "    <li>Footwear_Images</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from selenium import webdriver\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from os import listdir\n",
    "from matplotlib import image\n",
    "from random import shuffle \n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Model\n",
    "# tf.__version__\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg' # 'svg', 'retina'\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source URLs & Directory Folders for Image Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watches\n",
    "watch_url = 'https://www.amazon.com/s?i=specialty-aps&bbn=16225019011&rh=n%3A7141123011%2Cn%3A16225019011%2Cn%3A6358539011&pf_rd_i=16225019011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=5cd8272b-5ce4-4c26-bfcb-d6dca0c1e427&pf_rd_p=5cd8272b-5ce4-4c26-bfcb-d6dca0c1e427&pf_rd_r=CX53J0NV7EFDPJSMDE9S&pf_rd_r=CX53J0NV7EFDPJSMDE9S&pf_rd_s=merchandised-search-left-2&pf_rd_t=101&ref=AE_Men_Watches'\n",
    "#watch_dir = '../Assets/Watch_Images/'\n",
    "watch_dir = 'C:/Users/silve\\Desktop/materials-master/materials-master/DSI15 Capstone Project/Assets/Watch_Images/'\n",
    "\n",
    "# Shirts\n",
    "shirt_url = \"https://www.amazon.com/s?i=fashion-mens-intl-ship&bbn=16225019011&rh=n%3A16225019011%2Cn%3A1040658%2Cn%3A2476517011&dc&pf_rd_i=16225019011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=554625a3-8de1-4fdc-8877-99874d353388&pf_rd_r=SK809FT75R844KJ5WXGY&pf_rd_s=merchandised-search-4&pf_rd_t=101&qid=1594157542&rnid=1040658&ref=sr_nr_n_1\"\n",
    "#shirt_dir = '../Assets/Shirt_Images/'\n",
    "shirt_dir = 'C:/Users/silve\\Desktop/materials-master/materials-master/DSI15 Capstone Project/Assets/Shirt_Images/'\n",
    "\n",
    "# Footwear\n",
    "footwear_url = \"https://www.amazon.com/s?i=specialty-aps&bbn=16225019011&rh=n%3A7141123011%2Cn%3A16225019011%2Cn%3A679255011&pf_rd_i=16225019011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=5cd8272b-5ce4-4c26-bfcb-d6dca0c1e427&pf_rd_p=5cd8272b-5ce4-4c26-bfcb-d6dca0c1e427&pf_rd_r=V3G56PM79KZ6R1KBGHTK&pf_rd_r=V3G56PM79KZ6R1KBGHTK&pf_rd_s=merchandised-search-left-2&pf_rd_t=101&ref=AE_Men_Shoes\"\n",
    "#footwear_dir = '../Assets/Footwear_Images/'\n",
    "footwear_dir = 'C:/Users/silve\\Desktop/materials-master/materials-master/DSI15 Capstone Project/Assets/Footwear_Images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a general function to perform the web scrape (based on 10,000 image pull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amazon_scrape(url, directory):\n",
    "  image_num = 0\n",
    "  row = 1\n",
    "  page_response = driver.get(url)\n",
    "  page_content = soup(driver.page_source, 'html.parser')\n",
    "  images = page_content.findAll(\"img\",{\"class\":\"s-image\"})\n",
    "  image_num += len(images)\n",
    "  for i in range(len(images)):\n",
    "    f = open(directory+str((row-1)*(len(images))+i)+\".jpg\",'wb')\n",
    "    f.write(requests.get(images[i]['src']).content)\n",
    "    f.close\n",
    "  while(1):\n",
    "\n",
    "    row += 1\n",
    "    if(page_content.find(\"li\",{'class':'a-last'}) != None):\n",
    "      driver.find_element_by_xpath(\"//li[contains(@class, 'a-last')]/a\").click()\n",
    "      time.sleep(3)\n",
    "      page_content = soup(driver.page_source, 'html.parser')\n",
    "      images = page_content.findAll(\"img\",{\"class\":\"s-image\"})\n",
    "      image_num += len(images)\n",
    "      for i in range(len(images)):\n",
    "        f = open(directory+str((row-1)*(len(images))+i)+\".jpg\",'wb')\n",
    "        f.write(requests.get(images[i]['src']).content)\n",
    "        f.close\n",
    "      if(image_num > 10000): break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Images from the 3 respective product categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium run on chromedriver will be used to extract the images. Since there are 10,000 images per category, each pull will be done in a separate operation rather than a combined loop just in a case disruptions appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the chrome selenium webdriver\n",
    "driver = webdriver.Chrome(\"chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Images from the Amazon Category of Watches\n",
    "amazon_scrape(watch_url,watch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Images from the Amazon Category of T-shirts\n",
    "amazon_scrape(shirt_url,shirt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Images from the Amazon Category of Footwear\n",
    "amazon_scrape(footwear_url, footwear_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image data must be prepared before it can be used as the basis for modeling in image classification tasks.\n",
    "\n",
    "One aspect of preparing image data is scaling pixel values, such as normalizing the values to the range 0-1, centering, standardization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images from Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "path = \"C:/Users/silve/Desktop/materials-master/materials-master/DSI15 Capstone Project/Assets/\" \n",
    "image_size = 128\n",
    "types = [\"Watch_Images\", \"Shirt_Images\", \"Footwear_Images\"]\n",
    "labels = []\n",
    "dataset = []\n",
    "height = []\n",
    "width = []\n",
    "\n",
    "for i in types:\n",
    "    print(\"Reading the  data for: \", i)\n",
    "    for p in os.listdir(path + i):\n",
    "        image = cv2.imread(path + i + '/' + p)\n",
    "        height.append(image.shape[0])\n",
    "        width.append(image.shape[1])\n",
    "        image = cv2.resize(image, (image_size, image_size))  \n",
    "        image = image/255.0 # This scales each value to be between 0 and 1.\n",
    "        dataset.append(image) \n",
    "        labels.append(types.index(i))\n",
    "\n",
    "print(\"\\nNumber of Dataset {} and Number of Labels {}\".format(len(dataset),len(labels))) \n",
    "print(\"Single Image Shape:\", dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/silve/Desktop/materials-master/materials-master/DSI15 Capstone Project/Assets/\" \n",
    "image_size = 128\n",
    "channels = 3\n",
    "types = [\"Shirt_Images\", \"Shoe_Images\", \"Watch_Images\"]\n",
    "# labels = []\n",
    "# images = []\n",
    "\n",
    "total_images = 0\n",
    "for root, dirs, files in os.walk(path):\n",
    "    total_images += len(files)\n",
    "labels = np.empty(total_images)\n",
    "images = np.empty((total_images,image_size,image_size,channels))\n",
    "gray_images = np.empty((total_images,image_size,image_size))\n",
    "heights = np.empty(total_images)\n",
    "widths = np.empty(total_images) \n",
    "count = 0\n",
    "\n",
    "for i in types:\n",
    "    print(\"Reading the  data for: \", i)\n",
    "    for p in os.listdir(path + i):\n",
    "        image = cv2.imread(path + i + '/' + p)\n",
    "        heights[count] = image.shape[0]\n",
    "        widths[count] = image.shape[1]\n",
    "        #height.append(image.shape[0])\n",
    "        #width.append(image.shape[1])\n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "        #images.append(image) \n",
    "        #labels.append(types.index(i))\n",
    "        gray_images[count] = (cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))/255.2\n",
    "        image = image/255.0\n",
    "        images[count] = image\n",
    "        labels[count] = types.index(i)\n",
    "        \n",
    "        count+=1\n",
    "\n",
    "# print(\"\\nNumber of Dataset {} and Number of Labels {}\".format(len(images),len(labels))) \n",
    "# print(\"Single Image Shape:\", dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset)\n",
    "dataset_flattened = dataset.reshape(dataset.shape[0],-1)\n",
    "labels=np.array(labels)\n",
    "\n",
    "#Random shuffle the dataset and labels\n",
    "indices = np.arange(dataset.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "dataset = dataset[indices]\n",
    "labels = labels[indices]\n",
    "print(dataset.shape)\n",
    "print(dataset_flattened.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A preliminary visual inspections shows that there minimal image noise due from type inconsistency. There are images that are missclassified and out-of-category but these only comprise of a miniscule number of the data set. \n",
    "<ul>\n",
    "<li>The watch images are mostly consistent and of similar representation, but there appears to be many duplicate images. This will be further investigated.</li>\n",
    "<li>The shoe images are mostly clean, unique and of similar representation without much internal image noise. However, there is a mix of variations such as slippers, boots and sandals.</li>\n",
    "<li>The shirt category has a mix image representation. There are variations such as long & short sleeves, hoodies, vests, singlets and also human representation for some of the images.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check on Images Integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sample of images are inspected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = np.random.randint(0, len(dataset) - 1, size= 10)\n",
    "plt.figure(figsize=(15,15))\n",
    "for i, index in enumerate(index, 1):\n",
    "    img = dataset[index]\n",
    "    type_ind = labels[index]\n",
    "    title = types[type_ind]\n",
    "    plt.subplot(4, 5, i)\n",
    "    plt.title(title)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dimensions_df = pd.DataFrame({'height':height,'width':width})\n",
    "n,bins,_ = plt.hist(dimensions_df['height'],bins=5)\n",
    "plt.title('Height Distribution')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(dimensions_df['width'],bins=5)\n",
    "plt.title('Width Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_categorical = to_categorical(labels)\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels_categorical, test_size=1 - train_ratio, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_ratio/(test_ratio + validation_ratio), random_state=1)\n",
    "print(\"X Train shape:\", X_train.shape)\n",
    "print(\"X Test shape:\", X_test.shape)\n",
    "print(\"X Val shape:\", X_val.shape)\n",
    "print(\"Y Train shape:\", y_train.shape)\n",
    "print(\"Y Test shape:\", y_test.shape)\n",
    "print(\"Y Val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Image augmentation block\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=image_size + (3,), num_classes=2)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply pca on our dataset and see how dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(dataset_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1],hue=labels,palette=sns.color_palette(\"hls\", 3),legend=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(10,10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=pca_result[:,0], \n",
    "    ys=pca_result[:,1], \n",
    "    zs=pca_result[:,2], \n",
    "    c=labels\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For above projections, we can clearly see that pca fails, Lets apply t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne = TSNE(n_components=2).fit_transform(dataset_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(\n",
    "    x=tsne[:,0], y=tsne[:,1],\n",
    "    hue=labels,\n",
    "    palette=sns.color_palette(\"hls\", 3),\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Function to Load Images from Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def image_load(directory, list_name):\n",
    "\n",
    "    for filename in listdir(directory):\n",
    "    \n",
    "        # load image\n",
    "        img_data = image.imread(directory + filename)\n",
    "    \n",
    "        # store loaded image\n",
    "        list_name.append(img_data)\n",
    "    \n",
    "        print('Loaded %s %s' % (filename, img_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "watch_images = list()\n",
    "shirt_images = list()\n",
    "footwear_images = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Images\n",
    "image_load(watch_dir,watch_images)\n",
    "image_load(shirt_dir,shirt_images)\n",
    "image_load(footwear_dir,footwear_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Check that all images have been loaded in\n",
    "print('Watch images - {}'.format(len(watch_images)))\n",
    "print('Shirt images - {}'.format(len(shirt_images)))\n",
    "print('Footwear images - {}'.format(len(footwear_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display first few images from numy array to check integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = watch_images[:4]\n",
    "_, axs = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(x, axs):\n",
    "    ax.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = shirt_images[:4]\n",
    "_, axs = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(y, axs):\n",
    "    ax.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z = footwear_images[:4]\n",
    "_, axs = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(z, axs):\n",
    "    ax.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_images\n",
    "shirt_images\n",
    "footwear_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reading the dataset\n",
    "\n",
    "path = \"Dataset/\" \n",
    "image_size = 128\n",
    "types = [\"Shirt_Images\", \"Shoe_Images\", \"Watch_Images\"]\n",
    "labels = []\n",
    "dataset = []\n",
    "height = []\n",
    "width = []\n",
    "for i in types:\n",
    "    print(\"Reading the  data for: \", i)\n",
    "    for p in os.listdir(path + i):\n",
    "        image = cv2.imread(path + i + '/' + p)\n",
    "        height.append(image.shape[0])\n",
    "        width.append(image.shape[1])\n",
    "        image = cv2.resize(image, (image_size, image_size))\n",
    "        image = image/255.0\n",
    "        dataset.append(image) \n",
    "        labels.append(types.index(i))\n",
    "\n",
    "print(\"\\nNumber of Dataset {} and Number of Labels {}\".format(len(dataset),len(labels))) \n",
    "print(\"Single Image Shape:\", dataset[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of the average image resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_df = pd.DataFrame({'height':height,'width':width})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,bins,_ = plt.hist(dimensions_df['height'],bins=5)\n",
    "plt.show()\n",
    "plt.hist(dimensions_df['width'],bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_of_Shirt=types.index(\"Shirt_Images\")\n",
    "# index_of_Shoe=types.index(\"Shoe_Images\")\n",
    "# index_of_Watch=types.index(\"Watch_Images\")\n",
    "# total_Shirts=labels.count_nonzero(index_of_Shirt)\n",
    "# unique, shirts_counts = numpy.unique(a, return_counts=True)\n",
    "# total_Shoes=labels.count(index_of_Shoe)\n",
    "# total_Watches=labels.count(index_of_Watch)\n",
    "# print(\"Total Number of Shirts: \",total_Shirts)\n",
    "# print(\"Total Number of Shoes: \",total_Shoes)\n",
    "# print(\"Total Number of Watches: \",total_Watches)\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (180, 180)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory('Dataset', validation_split=0.2, subset='training', seed=1337,image_size=image_size, batch_size=batch_size,label_mode='categorical')\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory('Dataset', validation_split=0.2, subset='validation', seed=1337,image_size=image_size, batch_size=batch_size,label_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.fliplr(img) # Horizontal Flip\n",
    "image = np.flipup(img) # Vertical Flip\n",
    "image = rotate(img, angle = 45) # Rotate Angle\n",
    "image = random_noise(img) # Adding Noise (from skimage library)\n",
    "image = cv2.GaussianBlur(img, (11,11),0) # Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype('uint8'))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original, Average blur & Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy data for the image\n",
    "a = watch_images[0] \n",
    "\n",
    "# Show subplots | shape: (1,3) \n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    plt.sca(ax)\n",
    "    plt.imshow(a**(i+1), cmap=plt.cm.jet)\n",
    "    \n",
    "    #plt.colorbar()\n",
    "    plt.title('Image: {}'.format(i+1))\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.suptitle('Overall Title')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change images dimensions to 224 X 224 px "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import Image\n",
    "\n",
    "size = 128, 128\n",
    "\n",
    "for infile in sys.argv[1:]:\n",
    "    outfile = os.path.splitext(infile)[0] + \".thumbnail\"\n",
    "    if infile != outfile:\n",
    "        try:\n",
    "            im = Image.open(infile)\n",
    "            im.thumbnail(size, Image.ANTIALIAS)\n",
    "            im.save(outfile, \"JPEG\")\n",
    "        except IOError:\n",
    "            print \"cannot create thumbnail for '%s'\" % infile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "basewidth = 300\n",
    "img = Image.open('somepic.jpg')\n",
    "wpercent = (basewidth/float(img.size[0]))\n",
    "hsize = int((float(img.size[1])*float(wpercent)))\n",
    "img = img.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "img.save('sompic.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of saving a grayscale version of a loaded image\n",
    "from PIL import Image\n",
    "# load the image\n",
    "image = Image.open('opera_house.jpg')\n",
    "# convert the image to grayscale\n",
    "gs_image = image.convert(mode='L')\n",
    "# save in jpeg format\n",
    "gs_image.save('opera_house_grayscale.jpg')\n",
    "# load the image again and show it\n",
    "image2 = Image.open('opera_house_grayscale.jpg')\n",
    "# show the image\n",
    "image2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/silve\\Desktop/materials-master/materials-master/DSI15 Capstone Project/Assets/Train'\n",
    "test_dir = 'C:/Users/silve\\Desktop/materials-master/materials-master/DSI15 Capstone Project/Assets/Test'\n",
    "IMG_SIZE = 50\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "base_dir = '<full path to dataset folder>/'\n",
    "for f in sorted(os.listdir(base_dir)):\n",
    "    if os.path.isdir(base_dir+f):\n",
    "        print(f\"{f} is a target class\")\n",
    "        for i in sorted(os.listdir(base_dir+f)):\n",
    "            print(f\"{i} is an input image path\")\n",
    "            X.append(base_dir+f+'/'+i)\n",
    "            y.append(f)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    dirname = train_dir + label + '\\\\'\n",
    "    for imgfile in os.listdir(dirname):\n",
    "        if(imgfile[0] == '.'):\n",
    "            pass\n",
    "        else:\n",
    "            image = load_img(dirname + imgfile, target_size=(100, 100, 3))\n",
    "            image_arr = img_to_array(image)\n",
    "            image_train.append(image_arr)\n",
    "            train_Y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA failed due to very high features to very low, info was lost etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(TRAIN_DIR,img)\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        training_data.append([np.array(img),np.array(label)])\n",
    "    shuffle(training_data)\n",
    "    np.save('train_data.npy', training_data)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data():\n",
    "    testing_data = []\n",
    "    for img in tqdm(os.listdir(TEST_DIR)):\n",
    "        path = os.path.join(TEST_DIR,img)\n",
    "        img_num = img.split('.')[0]\n",
    "        img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        testing_data.append([np.array(img), img_num])\n",
    "        \n",
    "    shuffle(testing_data)\n",
    "    np.save('test_data.npy', testing_data)\n",
    "    return testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 2, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import inception_v4\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "def preprocess_input(x):\n",
    "    x = np.divide(x, 255.0)\n",
    "    x = np.subtract(x, 1.0)\n",
    "    x = np.multiply(x, 2.0)\n",
    "    return x\n",
    "\n",
    "# This function comes from Google's ImageNet Preprocessing Script\n",
    "def central_crop(image, central_fraction):\n",
    "\t\"\"\"Crop the central region of the image.\n",
    "\tRemove the outer parts of an image but retain the central region of the image\n",
    "\talong each dimension. If we specify central_fraction = 0.5, this function\n",
    "\treturns the region marked with \"X\" in the below diagram.\n",
    "\t   --------\n",
    "\t  |        |\n",
    "\t  |  XXXX  |\n",
    "\t  |  XXXX  |\n",
    "\t  |        |   where \"X\" is the central 50% of the image.\n",
    "\t   --------\n",
    "\tArgs:\n",
    "\timage: 3-D array of shape [height, width, depth]\n",
    "\tcentral_fraction: float (0, 1], fraction of size to crop\n",
    "\tRaises:\n",
    "\tValueError: if central_crop_fraction is not within (0, 1].\n",
    "\tReturns:\n",
    "\t3-D array\n",
    "\t\"\"\"\n",
    "\tif central_fraction <= 0.0 or central_fraction > 1.0:\n",
    "\t\traise ValueError('central_fraction must be within (0, 1]')\n",
    "\tif central_fraction == 1.0:\n",
    "\t\treturn image\n",
    "\n",
    "\timg_shape = image.shape\n",
    "\tdepth = img_shape[2]\n",
    "\tfraction_offset = int(1 / ((1 - central_fraction) / 2.0))\n",
    "\tbbox_h_start = np.divide(img_shape[0], fraction_offset)\n",
    "\tbbox_w_start = np.divide(img_shape[1], fraction_offset)\n",
    "\n",
    "\tbbox_h_size = img_shape[0] - bbox_h_start * 2\n",
    "\tbbox_w_size = img_shape[1] - bbox_w_start * 2\n",
    "\n",
    "\timage = image[bbox_h_start:bbox_h_start+bbox_h_size, bbox_w_start:bbox_w_start+bbox_w_size]\n",
    "\treturn image\n",
    "\n",
    "def get_processed_image(img_path):\n",
    "\t# Load image and convert from BGR to RGB\n",
    "\tim = np.asarray(cv2.imread(img_path))[:,:,::-1]\n",
    "\tim = central_crop(im, 0.875)\n",
    "\tim = cv2.resize(im, (299, 299))\n",
    "\tim = preprocess_input(im)\n",
    "\tif K.image_dim_ordering() == \"th\":\n",
    "\t\tim = np.transpose(im, (2,0,1))\n",
    "\t\tim = im.reshape(-1,3,299,299)\n",
    "\telse:\n",
    "\t\tim = im.reshape(-1,299,299,3)\n",
    "\treturn im\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t# Create model and load pre-trained weights\n",
    "\tmodel = inception_v4.create_model(weights='imagenet')\n",
    "\n",
    "\t# Open Class labels dictionary. (human readable label given ID)\n",
    "\tclasses = eval(open('validation_utils/class_names.txt', 'r').read())\n",
    "\n",
    "\t# Load test image!\n",
    "\timg_path = 'elephant.jpg'\n",
    "\timg = get_processed_image(img_path)\n",
    "\n",
    "\t# Run prediction on test image\n",
    "\tpreds = model.predict(img)\n",
    "\tprint(\"Class is: \" + classes[np.argmax(preds)-1])\n",
    "\tprint(\"Certainty is: \" + str(preds[0][np.argmax(preds)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th ~/fb.resnet.torch/main.lua -nClasses 122 -nEpochs 100 -data ~/imageclassification/train/ -save ~/Desktop/imageclassification_c122e100b30t4g1 -batchSize 30 -nThreads 4 -nGPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications import Xception # TensorFlow ONLY\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "\thelp=\"path to the input image\")\n",
    "ap.add_argument(\"-model\", \"--model\", type=str, default=\"vgg16\",\n",
    "\thelp=\"name of pre-trained network to use\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that maps model names to their classes\n",
    "# inside Keras\n",
    "MODELS = {\n",
    "\t\"vgg16\": VGG16,\n",
    "\t\"vgg19\": VGG19,\n",
    "\t\"inception\": InceptionV3,\n",
    "\t\"xception\": Xception, # TensorFlow ONLY\n",
    "\t\"resnet\": ResNet50\n",
    "}\n",
    "# esnure a valid model name was supplied via command line argument\n",
    "if args[\"model\"] not in MODELS.keys():\n",
    "\traise AssertionError(\"The --model command line argument should \"\n",
    "\t\t\"be a key in the `MODELS` dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "from uuid import uuid4\n",
    "from flask import Flask, request, render_template, send_from_directory\n",
    "import ann\n",
    "import sys\n",
    "from config import *\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "sys.dont_write_bytecode=True\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "ALLOWED_EXTENSIONS = set(['jpg', 'jpeg'])\n",
    "APP_ROOT = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return render_template(\"upload.html\")\n",
    "\n",
    "@app.route(\"/similar\", methods=[\"POST\"])\n",
    "def upload():\n",
    "    target = os.path.join(APP_ROOT, UPLOAD_FOLDER)\n",
    "    print(target)\n",
    "    if not os.path.isdir(target):\n",
    "            os.mkdir(target)\n",
    "    else:\n",
    "        print(\"Couldn't create upload directory: {}\".format(target))\n",
    "    print(request.files.getlist(\"file\"))\n",
    "    for upload in request.files.getlist(\"file\"):\n",
    "        print(upload)\n",
    "        print(\"{} is the file name\".format(upload.filename))\n",
    "        filename = upload.filename\n",
    "        if allowed_file(filename):\n",
    "            destination = \"/\".join([target, filename])\n",
    "            print (\"Accept incoming file:\", filename)\n",
    "            print (\"Save it to:\", destination)\n",
    "            upload.save(destination)\n",
    "            uploaded_image = UPLOAD_FOLDER+filename\n",
    "            similar_images = ann.find_similar_images(uploaded_image)\n",
    "            similar_images = [image.split(\"/\")[1] for image in similar_images]\n",
    "            list_images = [filename]+ similar_images \n",
    "            print(list_images)\n",
    "    return render_template(\"similar.html\", image_names=list_images)\n",
    "\n",
    "@app.route('/upload/<filename>')\n",
    "def send_uploaded(filename):\n",
    "    return send_from_directory(UPLOAD_FOLDER, filename)\n",
    "\n",
    "@app.route('/similar/<filename>')\n",
    "def send_similar(filename):\n",
    "    return send_from_directory(DATASET_PATH, filename)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # app.run(port=8080, debug=True)\n",
    "    app.run(host= '0.0.0.0', port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
